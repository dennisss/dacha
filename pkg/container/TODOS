TODO: The manager must implement filtering of nodes based on bundle platforms.


Support for storing config files / secrets in the cluster:
- We already have a blob system for large objects but this has no ACLs and isn't very efficient for large files
- Each config is named.
- For 

First Pi to setup:
- 64-bit Raspbian
- Fan control via PWM pin 18
- USB for stream deck
- USB for NRF52 communication dongle
- HDMI DDC via I2C (/dev/i2c-1)
- Use GPIO 27 to perform identification
  - Thus cab be separate worker that is present on all devices.
  - 


General naming rules:
- All names can only contain a-z, 0-9 and hyphen.
- All names must start with a letter.
- Job names can additionally contain a '.'
- Node and worker ids are internally represented as 64-bit numbers but are encoded in string form using Crockfork base32 after little endian encoding
  - This implies that these numbers are randomly re-generated until we get a value whose first character in base32 is in the a-z range. 




Next steps:
- Need to make sure that queries to the metastore are resilient to cancelled requests while the server is turning down (as this doesn't go through the normal ClsuterResolver systme).
- Verify that metastore routes are locally cached (using )
- Need a ClusterRpcServer which validates that the host name sent is correct based on our env var.
- Document how to update the manager/metastore and nod binaries
  - Ideally want one command to update all system componenets.

Resource types to support:
- Discrete: There is a finite set of available resource instances
  - e.g. ports, usb devices
- Scalar: Can be arbitrarily sliced up
  - e.g. memory, CPU (for simpliciy)
  - Each type will have a min granularity associated with it.

Operations to support:
- On a single worker:
  - Restart it on the same node
  - Restart it on a random new node (drain the existing one and re-calculate).

Need to ensure that if a worker has persistent storage, we don't accidentally delete that.

Need to support filtering the node set based on device requirements
=> This will require that nodes export the devices connected to this (in the case of USB, this may change over time).

Observed bugs:
- Metastore doesn't seem to stop after getting a SIGINT

TODO: Add HTML pages to each node and worker:
- For node want to:
  - Display all running nodes and their states in a stable.

TODO: Figure out how to robustly export the logs generated by the node runtime itself.

TODO: If a node goes down, we need to support re-assigning the workers to other nodes.

Also need to do auto-deletion of non-exciting tables:
- NodeMetadata (or at least move the high frequency stuff like the heart beats into a separate table).
- WorkerStatus

Jobs I want to turn up initially:
- Fan control based on temperature
- Blinkstick thing

I want to support pinning workers to nodes:
- This means that I also want to support moving workers between nodes
- Can't move a worker with a given name over until the existing node stops running it.
- Set WorkerMetadata::draining to true
- Wait for Node to mark the worker as killed
  - Once a worker is drained, we will never re-use it.
  - That means that we don't need more syncronization.
    - But what about rollbacks?
    - Rolling back will 
- Then re-upload using  

Suppose we have a job that can't be fully started due to too few nodes.
- What do we do?
- Manager needs to watch the NodeMetadata.
  - More coarsely, we need to watch the expression 'SELECT id from NodeMetadata' for changes.

Simpler strategy:
- Manager maintains a list of in-complete jobs.
- Every five minutes we will check if that job can be improved.
- So on start up we do need to re-process all jobs to verify they don't have any issues.
- If a job comes in that requires a 


Does require some syncronization
- Either way we need syncronization when 
- 


- Would be nice to have names for 

- Each worker will have a randomly generated 64-bit index.
- 



Next steps:
- Support moving a worker from 1 node to another
  - We do need to know about which ones are dead.

- Technically I now have everything necessary to start it up without issues.
- Some useful modes:
  - Pin to machine
  - Pin to USB
  - 

- I still want to add:
  - USB 

- Alternative is to use 'zones'
  - A metastore must connect and use something.
- When initializing a node, pass in that id as a flag.
- 

Some notes:
- I don't want to inject via flags as that's more difficult to scale?
  - 

Requirements
- Need to support multiple metastore instances on 1 LAN to ensure that we can still test it offline.
  - Commands that need support:
    - Metastore binary
    - Node
    - 'cluster' CLI
    - Binaries like manager
  - Simple option is to assume that we have a cluster id 
- Let's fix all issues with getting metastore to run with more than one instance?
  - Would be nice to have some monitoring to observe 
- Have at least one metastore worker running
- Have the manager running
- Must be able to create new jobs that are auto-assigned to machines
- Must support pinning to specific machines (either through USB bindings or based on a specific machine constraints)
- This will enable bringing back the stream deck switcher app.

- For individual nodes:
  - Must be able to automatically reload workers when they die.
  - 
- Can start a node and 

Main next steps:
- Define library for EdgeSwitch
- Need health checks for restarting workers
- Get the stream deck re-deployed
- Define basic zigbee based NRF communication (so need two nodes)

